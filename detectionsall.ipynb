{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d66d3cd1-ef7b-4652-a547-d42693d950b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load Haar cascade classifiers\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')\n",
    "\n",
    "def detect_and_display(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, 'Face', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # Detect eyes within the face region\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray, scaleFactor=1.1, minNeighbors=10, minSize=(15, 15))\n",
    "        for (ex, ey, ew, eh) in eyes:\n",
    "            cv2.rectangle(roi_color, (ex, ey), (ex+ew, ey+eh), (0, 255, 0), 2)\n",
    "            cv2.putText(roi_color, 'Eye', (ex, ey-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "        # Detect smile within the face region, ensuring it is in the lower half of the face\n",
    "        smiles = smile_cascade.detectMultiScale(roi_gray, scaleFactor=1.8, minNeighbors=35, minSize=(25, 25))\n",
    "        for (sx, sy, sw, sh) in smiles:\n",
    "            # Ensure the detected smile is in the lower half of the face region\n",
    "            if sy > h // 2:\n",
    "                cv2.rectangle(roi_color, (sx, sy), (sx+sw, sy+sh), (0, 0, 255), 2)\n",
    "                cv2.putText(roi_color, 'Smile', (sx, sy-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "def main():\n",
    "    # Open video capture\n",
    "    cap = cv2.VideoCapture(\"D:\\\\intern\\\\WhatsApp Video 2024-06-23 at 22.28.22_7223d964.mp4\")  # Use the provided video file path\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = detect_and_display(frame)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the capture and close windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2715ede5-8c98-4c5f-a39c-3dff881ae670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the cascade\n",
    "profile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_profileface.xml')\n",
    "\n",
    "# Open video capture from the provided video file path\n",
    "video_path = r\"D:\\intern\\pro.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "else:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect profiles\n",
    "        profiles = profile_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "        # Draw rectangles around the detected profiles and add labels\n",
    "        for (x, y, w, h) in profiles:\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, 'Profile', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "        # Display the output\n",
    "        cv2.imshow('Profile Detection', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the capture and close windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6ce0985-eda2-4882-a61f-d2883398f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the cascade classifier for upper body detection\n",
    "upper_body_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_upperbody.xml')\n",
    "\n",
    "# Open video capture from the provided video file path\n",
    "video_path = r\"D:\\intern\\body.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "else:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect upper bodies\n",
    "        upper_bodies = upper_body_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "        # Draw rectangles around the detected upper bodies\n",
    "        for (x, y, w, h) in upper_bodies:\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, 'Upper Body', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "        # Display the output\n",
    "        cv2.imshow('Upper Body Detection', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the capture and close windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51e0e678-7448-40c5-aa9b-36779f96af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the cascade classifier for full body detection\n",
    "full_body_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_fullbody.xml')\n",
    "\n",
    "# Open video capture from the provided video file path\n",
    "video_path = r\"D:\\intern\\body.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "else:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect full bodies\n",
    "        full_bodies = full_body_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "        # Draw rectangles around the detected full bodies\n",
    "        for (x, y, w, h) in full_bodies:\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "            cv2.putText(frame, 'Full Body', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "\n",
    "        # Display the output\n",
    "        cv2.imshow('Full Body Detection', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the capture and close windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5c215f1-797a-40b0-ad18-576d0a857b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# URLs for YOLO model files and class names\n",
    "yolov3_weights_url = \"https://pjreddie.com/media/files/yolov3.weights\"\n",
    "yolov3_cfg_url = \"https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\"\n",
    "coco_names_url = \"https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names\"\n",
    "\n",
    "# Download YOLO model files and class names\n",
    "def download_file(url, filename):\n",
    "    response = requests.get(url)\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "download_file(yolov3_weights_url, 'yolov3.weights')\n",
    "download_file(yolov3_cfg_url, 'yolov3.cfg')\n",
    "download_file(coco_names_url, 'coco.names')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c9df326-c68c-4368-8fcf-82723e8695e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_cars_yolo(frame):\n",
    "    # Get dimensions of the frame\n",
    "    (height, width) = frame.shape[:2]\n",
    "\n",
    "    # Preprocess the frame and detect objects\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    # Loop over the detections\n",
    "    for detection in detections:\n",
    "        for obj in detection:\n",
    "            # Extract class ID, confidence, and bounding box coordinates\n",
    "            scores = obj[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5 and class_id == 2:  # Car class ID is 2\n",
    "                center_x = int(obj[0] * width)\n",
    "                center_y = int(obj[1] * height)\n",
    "                w = int(obj[2] * width)\n",
    "                h = int(obj[3] * height)\n",
    "\n",
    "                # Calculate top-left corner coordinates of bounding box\n",
    "                x = int(center_x - (w / 2))\n",
    "                y = int(center_y - (h / 2))\n",
    "\n",
    "                # Draw bounding box and label\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, \"Car\", (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b100d932-db7e-494b-b4bd-8e123d3bfed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO model and classes\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = f.read().strip().split(\"\\n\")\n",
    "\n",
    "# Function to perform car detection using YOLO\n",
    "def detect_cars_yolo(frame):\n",
    "    # Get dimensions of the frame\n",
    "    (height, width) = frame.shape[:2]\n",
    "\n",
    "    # Preprocess the frame and detect objects\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layer_names = net.getLayerNames()\n",
    "    unconnected_out_layers = net.getUnconnectedOutLayers()\n",
    "    \n",
    "    if isinstance(unconnected_out_layers, np.ndarray):\n",
    "        output_layers = [layer_names[i - 1] for i in unconnected_out_layers.flatten()]\n",
    "    else:\n",
    "        output_layers = [layer_names[unconnected_out_layers - 1]]\n",
    "    \n",
    "    detections = net.forward(output_layers)\n",
    "\n",
    "    # Loop over the detections\n",
    "    for detection in detections:\n",
    "        for obj in detection:\n",
    "            # Extract class ID, confidence, and bounding box coordinates\n",
    "            scores = obj[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > 0.5 and class_id == 2:  # Car class ID is 2\n",
    "                # Scale bounding box coordinates to frame dimensions\n",
    "                box = obj[0:4] * np.array([width, height, width, height])\n",
    "                (center_x, center_y, box_width, box_height) = box.astype(\"int\")\n",
    "                \n",
    "                # Calculate top-left corner coordinates of bounding box\n",
    "                x = int(center_x - (box_width / 2))\n",
    "                y = int(center_y - (box_height / 2))\n",
    "\n",
    "                # Draw bounding box and label\n",
    "                cv2.rectangle(frame, (x, y), (x + box_width, y + box_height), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, \"Car\", (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "# Open video capture from the provided video file path\n",
    "video_path = r\"C:\\Users\\VALU\\Downloads\\Yolo-Vehicle-Counter-master\\Yolo-Vehicle-Counter-master\\bridge.mp4\" # Replace with your video file path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "else:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Detect cars in the frame\n",
    "        frame_with_cars = detect_cars_yolo(frame)\n",
    "        # Display the output\n",
    "        cv2.imshow('Car Detection', frame_with_cars)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the capture and close windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6fe588f-acbe-41d3-9816-2dedb2bf61dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO model and classes\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = f.read().strip().split(\"\\n\")\n",
    "\n",
    "# Function to perform object detection using YOLO\n",
    "def detect_objects_yolo(frame):\n",
    "    # Get dimensions of the frame\n",
    "    (height, width) = frame.shape[:2]\n",
    "\n",
    "    # Preprocess the frame and detect objects\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layer_names = net.getLayerNames()\n",
    "    unconnected_out_layers = net.getUnconnectedOutLayers()\n",
    "\n",
    "    if isinstance(unconnected_out_layers, np.ndarray):\n",
    "        output_layers = [layer_names[i - 1] for i in unconnected_out_layers.flatten()]\n",
    "    else:\n",
    "        output_layers = [layer_names[unconnected_out_layers - 1]]\n",
    "\n",
    "    detections = net.forward(output_layers)\n",
    "\n",
    "    # Loop over the detections\n",
    "    for detection in detections:\n",
    "        for obj in detection:\n",
    "            # Extract class ID, confidence, and bounding box coordinates\n",
    "            scores = obj[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:  # You can adjust the confidence threshold if needed\n",
    "                # Scale bounding box coordinates to frame dimensions\n",
    "                box = obj[0:4] * np.array([width, height, width, height])\n",
    "                (center_x, center_y, box_width, box_height) = box.astype(\"int\")\n",
    "\n",
    "                # Calculate top-left corner coordinates of bounding box\n",
    "                x = int(center_x - (box_width / 2))\n",
    "                y = int(center_y - (box_height / 2))\n",
    "\n",
    "                # Draw bounding box and label\n",
    "                label = classes[class_id]\n",
    "                cv2.rectangle(frame, (x, y), (x + box_width, y + box_height), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, label, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "# Open video capture from the provided video file path\n",
    "video_path = r\"C:\\Users\\VALU\\Downloads\\Yolo-Vehicle-Counter-master\\Yolo-Vehicle-Counter-master\\bridge.mp4\"  # Replace with your video file path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "else:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Detect objects in the frame\n",
    "        frame_with_objects = detect_objects_yolo(frame)\n",
    "        # Display the output\n",
    "        cv2.imshow('Object Detection', frame_with_objects)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the capture and close windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e150d01-b1c2-45df-bf62-37c75d338083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO model and classes\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = f.read().strip().split(\"\\n\")\n",
    "\n",
    "# Function to perform object detection using YOLO\n",
    "def detect_objects_yolo(frame):\n",
    "    # Get dimensions of the frame\n",
    "    (height, width) = frame.shape[:2]\n",
    "\n",
    "    # Preprocess the frame and detect objects\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    detections = net.forward(output_layers)\n",
    "\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "\n",
    "    # Loop over the detections\n",
    "    for detection in detections:\n",
    "        for obj in detection:\n",
    "            # Extract class ID, confidence, and bounding box coordinates\n",
    "            scores = obj[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:  # Threshold for confidence\n",
    "                # Scale bounding box coordinates to frame dimensions\n",
    "                box = obj[0:4] * np.array([width, height, width, height])\n",
    "                (center_x, center_y, box_width, box_height) = box.astype(\"int\")\n",
    "\n",
    "                # Calculate top-left corner coordinates of bounding box\n",
    "                x = int(center_x - (box_width / 2))\n",
    "                y = int(center_y - (box_height / 2))\n",
    "\n",
    "                boxes.append([x, y, int(box_width), int(box_height)])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # Apply Non-Maxima Suppression to suppress weak, overlapping bounding boxes\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    for i in indices:\n",
    "        box = boxes[i]\n",
    "        (x, y, w, h) = box\n",
    "        label = str(classes[class_ids[i]])\n",
    "        confidence = confidences[i]\n",
    "        \n",
    "        # Draw bounding box and label\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"{label}: {confidence:.2f}\", (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "# Open video capture from the provided video file path\n",
    "video_path = r\"C:\\Users\\VALU\\Downloads\\Yolo-Vehicle-Counter-master\\Yolo-Vehicle-Counter-master\\bridge.mp4\" # Replace with your video file path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "else:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Detect objects in the frame\n",
    "        frame_with_objects = detect_objects_yolo(frame)\n",
    "        # Display the output\n",
    "        cv2.imshow('Object Detection', frame_with_objects)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the capture and close windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8832326d-ca1e-4a66-9325-4e7c90c46cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO model and classes\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = f.read().strip().split(\"\\n\")\n",
    "\n",
    "# Function to perform object detection using YOLO\n",
    "def detect_objects_yolo(frame):\n",
    "    # Get dimensions of the frame\n",
    "    (height, width) = frame.shape[:2]\n",
    "\n",
    "    # Preprocess the frame and detect objects\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    detections = net.forward(output_layers)\n",
    "\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "\n",
    "    # Loop over the detections\n",
    "    for detection in detections:\n",
    "        for obj in detection:\n",
    "            # Extract class ID, confidence, and bounding box coordinates\n",
    "            scores = obj[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:  # Threshold for confidence\n",
    "                # Scale bounding box coordinates to frame dimensions\n",
    "                box = obj[0:4] * np.array([width, height, width, height])\n",
    "                (center_x, center_y, box_width, box_height) = box.astype(\"int\")\n",
    "\n",
    "                # Calculate top-left corner coordinates of bounding box\n",
    "                x = int(center_x - (box_width / 2))\n",
    "                y = int(center_y - (box_height / 2))\n",
    "\n",
    "                boxes.append([x, y, int(box_width), int(box_height)])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # Apply Non-Maxima Suppression to suppress weak, overlapping bounding boxes\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    for i in indices:\n",
    "        box = boxes[i]\n",
    "        (x, y, w, h) = box\n",
    "        label = str(classes[class_ids[i]])\n",
    "        confidence = confidences[i]\n",
    "        \n",
    "        # Draw bounding box and label\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"{label}: {confidence:.2f}\", (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "# Open video capture from the provided video file path\n",
    "video_path = r\"D:\\intern\\fruits.mp4\"# Replace with your video file path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "else:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Detect objects in the frame\n",
    "        frame_with_objects = detect_objects_yolo(frame)\n",
    "        # Display the output\n",
    "        cv2.imshow('Object Detection', frame_with_objects)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the capture and close windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4487c10-cec5-435a-9674-c74ff52db450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
